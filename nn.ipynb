{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import tensorflow as K\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from keras.models import Model\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Conv1D, GlobalAveragePooling1D, Bidirectional\n",
    "\n",
    "## Changed from 0 --> 1 \n",
    "np.random.seed(1)\n",
    "SAFE_DIV = 0.0001\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "NB_CORES = 10\n",
    "FREQ_UPPER_BOUND = 100\n",
    "NEIGHBOR_UPPER_BOUND = 5\n",
    "WNL = WordNetLemmatizer()\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "MIN_WORD_OCCURRENCE = 100\n",
    "REPLACE_WORD = \"memento\"\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_FOLDS = 12\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_FILE = \"glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/spencerrafii/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Added this to my code \n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_nlp(string):\n",
    "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    return string\n",
    "\n",
    "def preprocess_model(string):\n",
    "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    string = ' '.join([cutter(w) for w in string.split()])\n",
    "    return string\n",
    "\n",
    "def get_token_features(q1, q2):\n",
    "    token_features = [0.0] * 10\n",
    "\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "    token_features[0] = common_word_count / (\n",
    "        min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (\n",
    "        max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (\n",
    "        min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (\n",
    "        max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (\n",
    "        min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (\n",
    "        max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens)) / 2\n",
    "    return token_features\n",
    "\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
    "    \n",
    "def extract_features_nlp(df):\n",
    "    df = df.copy()\n",
    "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess_nlp)\n",
    "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess_nlp)\n",
    "\n",
    "    print(\"token features...\")\n",
    "    token_features = df.apply(\n",
    "        lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"cwc_min\"] = list(map(lambda x: x[0], token_features))\n",
    "    df[\"cwc_max\"] = list(map(lambda x: x[1], token_features))\n",
    "    df[\"csc_min\"] = list(map(lambda x: x[2], token_features))\n",
    "    df[\"csc_max\"] = list(map(lambda x: x[3], token_features))\n",
    "    df[\"ctc_min\"] = list(map(lambda x: x[4], token_features))\n",
    "    df[\"ctc_max\"] = list(map(lambda x: x[5], token_features))\n",
    "    df[\"last_word_eq\"] = list(map(lambda x: x[6], token_features))\n",
    "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    df[\"abs_len_diff\"] = list(map(lambda x: x[8], token_features))\n",
    "    df[\"mean_len\"] = list(map(lambda x: x[9], token_features))\n",
    "\n",
    "    print(\"fuzzy features..\")\n",
    "    df[\"token_set_ratio\"] = df.apply(\n",
    "        lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"token_sort_ratio\"] = df.apply(\n",
    "        lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]),\n",
    "        axis=1)\n",
    "    df[\"fuzz_ratio\"] = df.apply(\n",
    "        lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_partial_ratio\"] = df.apply(\n",
    "        lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"longest_substr_ratio\"] = df.apply(\n",
    "        lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]),\n",
    "        axis=1)\n",
    "    return df\n",
    "\n",
    "def create_question_hash(train_df, test_df):\n",
    "    train_qs = np.dstack([train_df[\"question1\"],\n",
    "                          train_df[\"question2\"]]).flatten()\n",
    "    test_qs = np.dstack([test_df[\"question1\"], test_df[\"question2\"]]).flatten()\n",
    "    all_qs = np.append(train_qs, test_qs)\n",
    "    all_qs = pd.DataFrame(all_qs)[0].drop_duplicates()\n",
    "    all_qs.reset_index(inplace=True, drop=True)\n",
    "    question_dict = pd.Series(\n",
    "        all_qs.index.values, index=all_qs.values).to_dict()\n",
    "    return question_dict\n",
    "\n",
    "def get_hash(df, hash_dict):\n",
    "    df = df.copy()\n",
    "    df[\"qid1\"] = df[\"question1\"].map(hash_dict)\n",
    "    df[\"qid2\"] = df[\"question2\"].map(hash_dict)\n",
    "    return df.drop([\"question1\", \"question2\"], axis=1)\n",
    "\n",
    "def get_kcore_dict(df):\n",
    "    df = df.copy()\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(df.qid1)\n",
    "    edges = list(df[[\"qid1\", \"qid2\"]].to_records(index=False))\n",
    "    g.add_edges_from(edges)\n",
    "    g.remove_edges_from(g.selfloop_edges())\n",
    "    df_output = pd.DataFrame(data=g.nodes(), columns=[\"qid\"])\n",
    "    df_output[\"kcore\"] = 0\n",
    "    for k in range(2, NB_CORES + 1):\n",
    "        ck = nx.k_core(g, k=k).nodes()\n",
    "        print(\"kcore\", k)\n",
    "        df_output.ix[df_output.qid.isin(ck), \"kcore\"] = k\n",
    "\n",
    "    return df_output.to_dict()[\"kcore\"]\n",
    "\n",
    "def get_kcore_features(df, kcore_dict):\n",
    "    df = df.copy()\n",
    "    df[\"kcore1\"] = df[\"qid1\"].apply(lambda x: kcore_dict[x])\n",
    "    df[\"kcore2\"] = df[\"qid2\"].apply(lambda x: kcore_dict[x])\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_minmax(df, col):\n",
    "    df = df.copy()\n",
    "    sorted_features = np.sort(np.vstack([df[col + \"1\"], df[col + \"2\"]]).T)\n",
    "    df[\"min_\" + col] = sorted_features[:, 0]\n",
    "    df[\"max_\" + col] = sorted_features[:, 1]\n",
    "    return df.drop([col + \"1\", col + \"2\"], axis=1)\n",
    "\n",
    "\n",
    "def get_neighbors(train_df, test_df):\n",
    "    neighbors = defaultdict(set)\n",
    "    for df in [train_df, test_df]:\n",
    "        for q1, q2 in zip(df[\"qid1\"], df[\"qid2\"]):\n",
    "            neighbors[q1].add(q2)\n",
    "            neighbors[q2].add(q1)\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def get_neighbor_features(df, neighbors):\n",
    "    df = df.copy()\n",
    "    common_nc = df.apply(\n",
    "        lambda x: len(neighbors[x.qid1].intersection(neighbors[x.qid2])),\n",
    "        axis=1)\n",
    "    min_nc = df.apply(\n",
    "        lambda x: min(len(neighbors[x.qid1]), len(neighbors[x.qid2])), axis=1)\n",
    "    df[\"common_neighbor_ratio\"] = common_nc / min_nc\n",
    "    df[\"common_neighbor_count\"] = common_nc.apply(\n",
    "        lambda x: min(x, NEIGHBOR_UPPER_BOUND))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_freq_features(df, frequency_map):\n",
    "    df = df.copy()\n",
    "    df[\"freq1\"] = df[\"qid1\"].map(\n",
    "        lambda x: min(frequency_map[x], FREQ_UPPER_BOUND))\n",
    "    df[\"freq2\"] = df[\"qid2\"].map(\n",
    "        lambda x: min(frequency_map[x], FREQ_UPPER_BOUND))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")\n",
    "\n",
    "\n",
    "def preprocess(string, final=True):\n",
    "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    if final:\n",
    "        string = ' '.join([cutter(w) for w in string.split()])\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "\n",
    "def prepare(q):\n",
    "    new_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_memento = True\n",
    "    for w in q.split()[::-1]:\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q\n",
    "            new_memento = True\n",
    "        elif w not in STOP_WORDS:\n",
    "            if new_memento:\n",
    "                new_q = [\"memento\"] + new_q\n",
    "                new_memento = False\n",
    "            if is_numeric(w):\n",
    "                numbers_q = [w] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [w] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = \" \".join(new_q)\n",
    "    return new_q, set(surplus_q), set(numbers_q)\n",
    "\n",
    "\n",
    "def extract_features_model(df):\n",
    "    q1s = np.array([\"\"] * len(df), dtype=object)\n",
    "    q2s = np.array([\"\"] * len(df), dtype=object)\n",
    "    features = np.zeros((len(df), 4))\n",
    "\n",
    "    for i, (q1, q2) in enumerate(list(zip(df[\"question1\"], df[\"question2\"]))):\n",
    "        q1s[i], surplus1, numbers1 = prepare(q1)\n",
    "        q2s[i], surplus2, numbers2 = prepare(q2)\n",
    "        features[i, 0] = len(surplus1.intersection(surplus2))\n",
    "        features[i, 1] = len(surplus1.union(surplus2))\n",
    "        features[i, 2] = len(numbers1.intersection(numbers2))\n",
    "        features[i, 3] = len(numbers1.union(numbers2))\n",
    "\n",
    "    return q1s, q2s, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JUST FOR THE FIRST RUN, PROCESS FEATURES\n",
    "\n",
    "You can jump to [the later section](#START-HERE-IF-YOU-ALREADY-HAVE-THE-PROCESSED-FEATURES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_full_file = pd.read_csv('Def_Item_Diff_Paper.csv','\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>fold_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>81270</td>\n",
       "      <td>26253</td>\n",
       "      <td>26426</td>\n",
       "      <td>b'The extent to which a job requires the use o...</td>\n",
       "      <td>b'How do you rate the project and the software...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>89834</td>\n",
       "      <td>89834</td>\n",
       "      <td>67430</td>\n",
       "      <td>b'The extent to which an organization is willi...</td>\n",
       "      <td>b'People in our business are encouraged to tak...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>89926</td>\n",
       "      <td>50970</td>\n",
       "      <td>43688</td>\n",
       "      <td>b'The degree to which top management views IT ...</td>\n",
       "      <td>b'I could complete the job using a word proces...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>24687</td>\n",
       "      <td>24687</td>\n",
       "      <td>24305</td>\n",
       "      <td>b'The anticipated advantages that electronic d...</td>\n",
       "      <td>b'Improved accuracy.'</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26218</td>\n",
       "      <td>25709</td>\n",
       "      <td>25702</td>\n",
       "      <td>b'A subjective feeling of being together with ...</td>\n",
       "      <td>b'The extent of use of IT including the Intern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id   qid1   qid2  \\\n",
       "0           0  81270  26253  26426   \n",
       "1           1  89834  89834  67430   \n",
       "2           2  89926  50970  43688   \n",
       "3           3  24687  24687  24305   \n",
       "4           4  26218  25709  25702   \n",
       "\n",
       "                                           question1  \\\n",
       "0  b'The extent to which a job requires the use o...   \n",
       "1  b'The extent to which an organization is willi...   \n",
       "2  b'The degree to which top management views IT ...   \n",
       "3  b'The anticipated advantages that electronic d...   \n",
       "4  b'A subjective feeling of being together with ...   \n",
       "\n",
       "                                           question2  is_duplicate  fold_id  \n",
       "0  b'How do you rate the project and the software...             0      0.0  \n",
       "1  b'People in our business are encouraged to tak...             1      2.0  \n",
       "2  b'I could complete the job using a word proces...             0      4.0  \n",
       "3                              b'Improved accuracy.'             1      4.0  \n",
       "4  b'The extent of use of IT including the Intern...             0      0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = []\n",
    "test_df =[]\n",
    "for row in df_full_file.itertuples():\n",
    "        if row[8] == 4.0:\n",
    "            test_df.append([row[2],row[3],row[4],row[5],row[6],row[7]])\n",
    "        else: \n",
    "            train_df.append([row[2],row[3],row[4],row[5],row[6],row[7]])\n",
    "test_df = pd.DataFrame(test_df, columns = [\"id\",'qid1','qid2','question1','question2','is_duplicate'])\n",
    "train_df = pd.DataFrame(train_df, columns = [\"id\",'qid1','qid2','question1','question2','is_duplicate'])\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89926</td>\n",
       "      <td>50970</td>\n",
       "      <td>43688</td>\n",
       "      <td>b'The degree to which top management views IT ...</td>\n",
       "      <td>b'I could complete the job using a word proces...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24687</td>\n",
       "      <td>24687</td>\n",
       "      <td>24305</td>\n",
       "      <td>b'The anticipated advantages that electronic d...</td>\n",
       "      <td>b'Improved accuracy.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33493</td>\n",
       "      <td>427</td>\n",
       "      <td>1122</td>\n",
       "      <td>b'The satisfaction with benefits from outsourc...</td>\n",
       "      <td>b'Overall, the quality of this meeting was high.'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>905</td>\n",
       "      <td>b\"The extent to which information systems staf...</td>\n",
       "      <td>b'Were there important opinion differences bet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>1242</td>\n",
       "      <td>b\"The respondents' feeling of computer dominan...</td>\n",
       "      <td>b'I usually have to make my work fit the compu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   qid1   qid2                                          question1  \\\n",
       "0  89926  50970  43688  b'The degree to which top management views IT ...   \n",
       "1  24687  24687  24305  b'The anticipated advantages that electronic d...   \n",
       "2  33493    427   1122  b'The satisfaction with benefits from outsourc...   \n",
       "3    319    319    905  b\"The extent to which information systems staf...   \n",
       "4    486    486   1242  b\"The respondents' feeling of computer dominan...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  b'I could complete the job using a word proces...             0  \n",
       "1                              b'Improved accuracy.'             1  \n",
       "2  b'Overall, the quality of this meeting was high.'             0  \n",
       "3  b'Were there important opinion differences bet...             1  \n",
       "4  b'I usually have to make my work fit the compu...             1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for train:\n",
      "token features...\n",
      "fuzzy features..\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features for train:\")\n",
    "train_df_nlp = extract_features_nlp(train_df)\n",
    "train_df_nlp.drop(\n",
    "    [\"id\", \"qid1\", \"qid2\", \"question1\", \"question2\", \"is_duplicate\"],\n",
    "    axis=1,\n",
    "    inplace=True)\n",
    "# train_df_nlp.to_csv(\"data/nlp_features_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for test:\n",
      "token features...\n",
      "fuzzy features..\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "labels ['test_id'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f7cb7a9af663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting features for test:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_df_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_df_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# test_df_nlp.to_csv(\"data/nlp_features_test.csv\", index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2048\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2051\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3574\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3575\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3576\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3577\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['test_id'] not contained in axis"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features for test:\")\n",
    "test_df_nlp = extract_features_nlp(test_df)\n",
    "test_df_nlp.drop([\"test_id\", \"question1\", \"question2\"], axis=1, inplace=True)\n",
    "# test_df_nlp.to_csv(\"data/nlp_features_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81270</td>\n",
       "      <td>26253</td>\n",
       "      <td>26426</td>\n",
       "      <td>b'The extent to which a job requires the use o...</td>\n",
       "      <td>b'How do you rate the project and the software...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89834</td>\n",
       "      <td>89834</td>\n",
       "      <td>67430</td>\n",
       "      <td>b'The extent to which an organization is willi...</td>\n",
       "      <td>b'People in our business are encouraged to tak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26218</td>\n",
       "      <td>25709</td>\n",
       "      <td>25702</td>\n",
       "      <td>b'A subjective feeling of being together with ...</td>\n",
       "      <td>b'The extent of use of IT including the Intern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45451</td>\n",
       "      <td>23876</td>\n",
       "      <td>43825</td>\n",
       "      <td>b'Expectations of physical facilities, equipme...</td>\n",
       "      <td>b'How important do you think how quickly you w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62767</td>\n",
       "      <td>81760</td>\n",
       "      <td>60983</td>\n",
       "      <td>b'Formal career planning programs and senior m...</td>\n",
       "      <td>b'I get the information I need in time.'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   qid1   qid2                                          question1  \\\n",
       "0  81270  26253  26426  b'The extent to which a job requires the use o...   \n",
       "1  89834  89834  67430  b'The extent to which an organization is willi...   \n",
       "2  26218  25709  25702  b'A subjective feeling of being together with ...   \n",
       "3  45451  23876  43825  b'Expectations of physical facilities, equipme...   \n",
       "4  62767  81760  60983  b'Formal career planning programs and senior m...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  b'How do you rate the project and the software...             0  \n",
       "1  b'People in our business are encouraged to tak...             1  \n",
       "2  b'The extent of use of IT including the Intern...             0  \n",
       "3  b'How important do you think how quickly you w...             0  \n",
       "4           b'I get the information I need in time.'             0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>token_set_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.285710</td>\n",
       "      <td>0.166665</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.166665</td>\n",
       "      <td>0.307690</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>20.0</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.058823</td>\n",
       "      <td>0.199996</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>0.071428</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>0.034188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142855</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.230767</td>\n",
       "      <td>0.142856</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>17.0</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.142855</td>\n",
       "      <td>0.142855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.153845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>0.124999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12.5</td>\n",
       "      <td>53</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>50</td>\n",
       "      <td>0.317073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cwc_min   cwc_max   csc_min   csc_max   ctc_min   ctc_max  last_word_eq  \\\n",
       "0  0.285710  0.166665  0.399992  0.166665  0.307690  0.148148             0   \n",
       "1  0.083333  0.058823  0.199996  0.111110  0.111110  0.071428             0   \n",
       "2  0.142855  0.083333  0.399992  0.399992  0.230767  0.142856             0   \n",
       "3  0.142855  0.142855  0.000000  0.000000  0.099999  0.062500             0   \n",
       "4  0.399992  0.153845  0.000000  0.000000  0.222220  0.124999             0   \n",
       "\n",
       "   first_word_eq  abs_len_diff  mean_len  token_set_ratio  token_sort_ratio  \\\n",
       "0              1            14      20.0               33                28   \n",
       "1              1            10      23.0               25                26   \n",
       "2              1             8      17.0               40                42   \n",
       "3              1             6      13.0               23                21   \n",
       "4              1             7      12.5               53                27   \n",
       "\n",
       "   fuzz_ratio  fuzz_partial_ratio  longest_substr_ratio  \n",
       "0          37                  43              0.117647  \n",
       "1          22                  35              0.034188  \n",
       "2          44                  42              0.075000  \n",
       "3          25                  29              0.038462  \n",
       "4          35                  50              0.317073  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89926</td>\n",
       "      <td>50970</td>\n",
       "      <td>43688</td>\n",
       "      <td>b'The degree to which top management views IT ...</td>\n",
       "      <td>b'I could complete the job using a word proces...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24687</td>\n",
       "      <td>24687</td>\n",
       "      <td>24305</td>\n",
       "      <td>b'The anticipated advantages that electronic d...</td>\n",
       "      <td>b'Improved accuracy.'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33493</td>\n",
       "      <td>427</td>\n",
       "      <td>1122</td>\n",
       "      <td>b'The satisfaction with benefits from outsourc...</td>\n",
       "      <td>b'Overall, the quality of this meeting was high.'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>905</td>\n",
       "      <td>b\"The extent to which information systems staf...</td>\n",
       "      <td>b'Were there important opinion differences bet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>1242</td>\n",
       "      <td>b\"The respondents' feeling of computer dominan...</td>\n",
       "      <td>b'I usually have to make my work fit the compu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   qid1   qid2                                          question1  \\\n",
       "0  89926  50970  43688  b'The degree to which top management views IT ...   \n",
       "1  24687  24687  24305  b'The anticipated advantages that electronic d...   \n",
       "2  33493    427   1122  b'The satisfaction with benefits from outsourc...   \n",
       "3    319    319    905  b\"The extent to which information systems staf...   \n",
       "4    486    486   1242  b\"The respondents' feeling of computer dominan...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  b'I could complete the job using a word proces...             0  \n",
       "1                              b'Improved accuracy.'             1  \n",
       "2  b'Overall, the quality of this meeting was high.'             0  \n",
       "3  b'Were there important opinion differences bet...             1  \n",
       "4  b'I usually have to make my work fit the compu...             1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>...</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>token_set_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89926</td>\n",
       "      <td>50970</td>\n",
       "      <td>43688</td>\n",
       "      <td>b the degree to which top management views it ...</td>\n",
       "      <td>b i could complete the job using a word proces...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142855</td>\n",
       "      <td>0.090908</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.142855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>16.5</td>\n",
       "      <td>33</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24687</td>\n",
       "      <td>24687</td>\n",
       "      <td>24305</td>\n",
       "      <td>b the anticipated advantages that electronic d...</td>\n",
       "      <td>b improved accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076922</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33493</td>\n",
       "      <td>427</td>\n",
       "      <td>1122</td>\n",
       "      <td>b the satisfaction with benefits from outsourc...</td>\n",
       "      <td>b overall  the quality of this meeting was high</td>\n",
       "      <td>0</td>\n",
       "      <td>0.199996</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157894</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>905</td>\n",
       "      <td>b the extent to which information systems staf...</td>\n",
       "      <td>b were there important opinion differences bet...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.499995</td>\n",
       "      <td>0.357140</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>0.249997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279999</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "      <td>0.089552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>486</td>\n",
       "      <td>486</td>\n",
       "      <td>1242</td>\n",
       "      <td>b the respondents  feeling of computer dominan...</td>\n",
       "      <td>b i usually have to make my work fit the compu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.285710</td>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176470</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>12.0</td>\n",
       "      <td>46</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>38</td>\n",
       "      <td>0.196078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   qid1   qid2                                          question1  \\\n",
       "0  89926  50970  43688  b the degree to which top management views it ...   \n",
       "1  24687  24687  24305  b the anticipated advantages that electronic d...   \n",
       "2  33493    427   1122  b the satisfaction with benefits from outsourc...   \n",
       "3    319    319    905  b the extent to which information systems staf...   \n",
       "4    486    486   1242  b the respondents  feeling of computer dominan...   \n",
       "\n",
       "                                           question2  is_duplicate   cwc_min  \\\n",
       "0  b i could complete the job using a word proces...             0  0.142855   \n",
       "1                              b improved accuracy               1  0.333322   \n",
       "2  b overall  the quality of this meeting was high               0  0.199996   \n",
       "3  b were there important opinion differences bet...             1  0.499995   \n",
       "4  b i usually have to make my work fit the compu...             1  0.399992   \n",
       "\n",
       "    cwc_max   csc_min   csc_max          ...            ctc_max  last_word_eq  \\\n",
       "0  0.090908  0.166664  0.142855          ...           0.100000             0   \n",
       "1  0.111110  0.000000  0.000000          ...           0.076922             0   \n",
       "2  0.111110  0.499988  0.249997          ...           0.157894             0   \n",
       "3  0.357140  0.249997  0.249997          ...           0.279999             0   \n",
       "4  0.285710  0.499975  0.166664          ...           0.176470             0   \n",
       "\n",
       "   first_word_eq  abs_len_diff  mean_len  token_set_ratio  token_sort_ratio  \\\n",
       "0              1             7      16.5               33                37   \n",
       "1              1            10       8.0               22                21   \n",
       "2              1            10      14.0               35                29   \n",
       "3              1             4      23.0               48                53   \n",
       "4              1            10      12.0               46                31   \n",
       "\n",
       "   fuzz_ratio  fuzz_partial_ratio  longest_substr_ratio  \n",
       "0          27                  35              0.066667  \n",
       "1          17                  38              0.181818  \n",
       "2          23                  39              0.100000  \n",
       "3          33                  38              0.089552  \n",
       "4          28                  38              0.196078  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing the questions...\n",
      "Number of unique questions: 18196\n"
     ]
    }
   ],
   "source": [
    "print(\"Hashing the questions...\")\n",
    "question_dict = create_question_hash(train_df, test_df)\n",
    "train_df_non = get_hash(train_df, question_dict)\n",
    "test_df_non = get_hash(test_df, question_dict)\n",
    "print(\"Number of unique questions:\", len(question_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating kcore features...\n",
      "kcore 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:123: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcore 3\n",
      "kcore 4\n",
      "kcore 5\n",
      "kcore 6\n",
      "kcore 7\n",
      "kcore 8\n",
      "kcore 9\n",
      "kcore 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating kcore features...\")\n",
    "all_df = pd.concat([train_df_non, test_df_non])\n",
    "kcore_dict = get_kcore_dict(all_df)\n",
    "train_df_non = get_kcore_features(train_df_non, kcore_dict)\n",
    "test_df_non = get_kcore_features(test_df_non, kcore_dict)\n",
    "train_df_non = convert_to_minmax(train_df_non, \"kcore\")\n",
    "test_df_non = convert_to_minmax(test_df_non, \"kcore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating common neighbor features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating common neighbor features...\")\n",
    "neighbors = get_neighbors(train_df_non, test_df_non)\n",
    "train_df_non = get_neighbor_features(train_df_non, neighbors)\n",
    "test_df_non = get_neighbor_features(test_df_non, neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating frequency features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating frequency features...\")\n",
    "frequency_map = dict(\n",
    "    zip(*np.unique(\n",
    "        np.vstack((all_df[\"qid1\"], all_df[\"qid2\"])), return_counts=True)))\n",
    "train_df_non = get_freq_features(train_df_non, frequency_map)\n",
    "test_df_non = get_freq_features(test_df_non, frequency_map)\n",
    "train_df_non = convert_to_minmax(train_df_non, \"freq\")\n",
    "test_df_non = convert_to_minmax(test_df_non, \"freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"min_kcore\", \"max_kcore\", \"common_neighbor_count\", \"common_neighbor_ratio\",\n",
    "    \"min_freq\", \"max_freq\"\n",
    "]\n",
    "train_df_non = train_df_non[cols] # .to_csv(\"non_nlp_features_train.csv\", index=False)\n",
    "test_df_non = test_df_non[cols] # .to_csv(\"non_nlp_features_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_kcore</th>\n",
       "      <th>max_kcore</th>\n",
       "      <th>common_neighbor_count</th>\n",
       "      <th>common_neighbor_ratio</th>\n",
       "      <th>min_freq</th>\n",
       "      <th>max_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_kcore  max_kcore  common_neighbor_count  common_neighbor_ratio  \\\n",
       "0          2          2                      0                    0.0   \n",
       "1          2          2                      0                    0.0   \n",
       "2          2          2                      0                    0.0   \n",
       "3          2          2                      0                    0.0   \n",
       "4          2          2                      0                    0.0   \n",
       "\n",
       "   min_freq  max_freq  \n",
       "0         2        10  \n",
       "1         3         9  \n",
       "2         2        10  \n",
       "3         3        10  \n",
       "4         3         5  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_non.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_kcore</th>\n",
       "      <th>max_kcore</th>\n",
       "      <th>common_neighbor_count</th>\n",
       "      <th>common_neighbor_ratio</th>\n",
       "      <th>min_freq</th>\n",
       "      <th>max_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_kcore  max_kcore  common_neighbor_count  common_neighbor_ratio  \\\n",
       "0          0          2                      0                    0.0   \n",
       "1          2          2                      0                    0.0   \n",
       "2          0          2                      0                    0.0   \n",
       "3          2          2                      0                    0.0   \n",
       "4          2          2                      0                    0.0   \n",
       "\n",
       "   min_freq  max_freq  \n",
       "0         2         7  \n",
       "1         2        20  \n",
       "2         2        19  \n",
       "3         2         7  \n",
       "4         3         7  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_non.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_test = test_df_nlp\n",
    "features_train = train_df_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22124, 15), (5530, 21))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape, features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('_train', features_train)\n",
    "np.save('_test', features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-058d1d044437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE IF YOU ALREADY HAVE THE PROCESSED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "test =[]\n",
    "for row in df_full_file.itertuples():\n",
    "        if row[8] == 4.0:\n",
    "            test.append([row[2],row[3],row[4],row[5],row[6],row[7]])\n",
    "        else: \n",
    "            train.append([row[2],row[3],row[4],row[5],row[6],row[7]])\n",
    "test = pd.DataFrame(test, columns = [\"id\",'qid1','qid2','question1','question2','is_duplicate'])\n",
    "train = pd.DataFrame(train, columns = [\"id\",'qid1','qid2','question1','question2','is_duplicate'])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              22124\n",
       "qid1            22124\n",
       "qid2            22124\n",
       "question1       22124\n",
       "question2       22124\n",
       "is_duplicate    22124\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              5530\n",
       "qid1            5530\n",
       "qid2            5530\n",
       "question1       5530\n",
       "question2       5530\n",
       "is_duplicate    5530\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_questions = pd.Series(\n",
    "    train[\"question1\"].tolist() + train[\"question2\"].tolist()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81270</td>\n",
       "      <td>26253</td>\n",
       "      <td>26426</td>\n",
       "      <td>b'The extent to which a job requires the use o...</td>\n",
       "      <td>b'How do you rate the project and the software...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89834</td>\n",
       "      <td>89834</td>\n",
       "      <td>67430</td>\n",
       "      <td>b'The extent to which an organization is willi...</td>\n",
       "      <td>b'People in our business are encouraged to tak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26218</td>\n",
       "      <td>25709</td>\n",
       "      <td>25702</td>\n",
       "      <td>b'A subjective feeling of being together with ...</td>\n",
       "      <td>b'The extent of use of IT including the Intern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45451</td>\n",
       "      <td>23876</td>\n",
       "      <td>43825</td>\n",
       "      <td>b'Expectations of physical facilities, equipme...</td>\n",
       "      <td>b'How important do you think how quickly you w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62767</td>\n",
       "      <td>81760</td>\n",
       "      <td>60983</td>\n",
       "      <td>b'Formal career planning programs and senior m...</td>\n",
       "      <td>b'I get the information I need in time.'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26990</td>\n",
       "      <td>25559</td>\n",
       "      <td>25435</td>\n",
       "      <td>b'Commitment to knowledge management by\\nthe t...</td>\n",
       "      <td>b'The tools provided by the site allow  me to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35852</td>\n",
       "      <td>88970</td>\n",
       "      <td>66203</td>\n",
       "      <td>b'The extent to which the company has an appro...</td>\n",
       "      <td>b'My chances of losing data in the future are.'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30402</td>\n",
       "      <td>28439</td>\n",
       "      <td>28676</td>\n",
       "      <td>b'General agreement concerning development pri...</td>\n",
       "      <td>b'Methodologies and tools for OO analysis, des...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27197</td>\n",
       "      <td>25939</td>\n",
       "      <td>25933</td>\n",
       "      <td>b\"Participants' intention to shop online.\"</td>\n",
       "      <td>b'If I use a computer I will be seen as higher...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>89353</td>\n",
       "      <td>89353</td>\n",
       "      <td>66684</td>\n",
       "      <td>b'How well individual employees perform altrui...</td>\n",
       "      <td>b'(Name of employee) helps others in the work ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   qid1   qid2                                          question1  \\\n",
       "0  81270  26253  26426  b'The extent to which a job requires the use o...   \n",
       "1  89834  89834  67430  b'The extent to which an organization is willi...   \n",
       "2  26218  25709  25702  b'A subjective feeling of being together with ...   \n",
       "3  45451  23876  43825  b'Expectations of physical facilities, equipme...   \n",
       "4  62767  81760  60983  b'Formal career planning programs and senior m...   \n",
       "5  26990  25559  25435  b'Commitment to knowledge management by\\nthe t...   \n",
       "6  35852  88970  66203  b'The extent to which the company has an appro...   \n",
       "7  30402  28439  28676  b'General agreement concerning development pri...   \n",
       "8  27197  25939  25933         b\"Participants' intention to shop online.\"   \n",
       "9  89353  89353  66684  b'How well individual employees perform altrui...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  b'How do you rate the project and the software...             0  \n",
       "1  b'People in our business are encouraged to tak...             1  \n",
       "2  b'The extent of use of IT including the Intern...             0  \n",
       "3  b'How important do you think how quickly you w...             0  \n",
       "4           b'I get the information I need in time.'             0  \n",
       "5  b'The tools provided by the site allow  me to ...             0  \n",
       "6    b'My chances of losing data in the future are.'             0  \n",
       "7  b'Methodologies and tools for OO analysis, des...             0  \n",
       "8  b'If I use a computer I will be seen as higher...             0  \n",
       "9  b'(Name of employee) helps others in the work ...             1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'The extent to which a job requires the use of different talents.'\"\n",
      " \"b'The extent to which an organization is willing to commit resources to projects, activities,\\\\nand solutions that contain a high level of uncertainty regarding their likely outcomes.'\"\n",
      " \"b'A subjective feeling of being together with others in a virtual environment.'\"\n",
      " ...\n",
      " \"b'Using this Web site is: will never recommend - will definitely recommend.'\"\n",
      " \"b'My department encourages finding new methods to perform a task.'\"\n",
      " \"b'Improved access to suppliers\\\\xc2\\\\x92 price and product descriptions.'\"]\n"
     ]
    }
   ],
   "source": [
    "print(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I should make this into a function to test my parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw datasets...\n",
      "Loaded.\n",
      "Creating the vocabulary of words occurred more than 100\n",
      "Words are not found in the embedding: set()\n",
      "Train questions are being prepared for LSTM...\n",
      "Same steps are being applied for test...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading raw datasets...\")\n",
    "# train_df = pd.read_csv(\"../data/train.csv\")\n",
    "# test_df = pd.read_csv(\"../data/test.csv\")\n",
    "train_df =  train_df_non\n",
    "test_df = test_df_non\n",
    "print(\"Loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "train[\"question1\"] = train[\"question1\"].fillna(\"\").apply(preprocess_model)\n",
    "train[\"question2\"] = train[\"question2\"].fillna(\"\").apply(preprocess_model)\n",
    "\n",
    "print(\"Creating the vocabulary of words occurred more than\",\n",
    "      MIN_WORD_OCCURRENCE)\n",
    "all_questions = pd.Series(\n",
    "    train[\"question1\"].tolist() + train[\"question2\"].tolist()).unique()\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=False, token_pattern=\"\\S+\", min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(all_questions)\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(REPLACE_WORD)\n",
    "\n",
    "embeddings_index = get_embedding()\n",
    "print(\"Words are not found in the embedding:\",\n",
    "      top_words - embeddings_index.keys())\n",
    "top_words = embeddings_index.keys()\n",
    "\n",
    "print(\"Train questions are being prepared for LSTM...\")\n",
    "q1s_train, q2s_train, train_q_features = extract_features_model(train)\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(q1s_train, q2s_train))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "data_1 = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(q1s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(q2s_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train[\"is_duplicate\"])\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "print(\"Same steps are being applied for test...\")\n",
    "test[\"question1\"] = test[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "test[\"question2\"] = test[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "q1s_test, q2s_test, test_q_features = extract_features_model(test)\n",
    "test_data_1 = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(q1s_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(q2s_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "features_train = np.load(\"_train.npy\")\n",
    "features_test = np.load(\"_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Validation data from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx_test, idx_val in skf.split(test[\"is_duplicate\"], test[\"is_duplicate\"]):\n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    labels_val = labels[idx_val]\n",
    "    f_val = features_train[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1105, 32), (1105, 32), (1105,), (1105, 15))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_val.shape, data_2_val.shape, labels_val.shape, f_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_1_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d0d63d521a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_1_train' is not defined"
     ]
    }
   ],
   "source": [
    "data_1_train.shape, data_2_train.shape, labels_train.shape, f_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_count = 0\n",
    "for idx_train, idx_val in skf.split(train[\"is_duplicate\"], train[\"is_duplicate\"]):\n",
    "    print(\"MODEL:\", model_count)\n",
    "    data_1_train = data_1[idx_train]\n",
    "    data_2_train = data_2[idx_train]\n",
    "    labels_train = labels[idx_train]\n",
    "    f_train = features_train[idx_train]\n",
    "\n",
    "    data_1_val = data_1[idx_val]\n",
    "    data_2_val = data_2[idx_val]\n",
    "    labels_val = labels[idx_val]\n",
    "    f_val = features_train[idx_val]\n",
    "\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    ## Changed recurrent_dropout = .2 --> 0\n",
    "    lstm_layer = LSTM(96, recurrent_dropout=0.0)\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    features_input = Input(shape=(f_train.shape[1],), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.0)(features_dense) ## Change from .25\n",
    "    \n",
    "\n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged) #changed from .2\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\")\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    best_model_path = \"__best_model\" + str(model_count) + \".h5\"\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    hist = model.fit([data_1_train, data_2_train, f_train], labels_train,\n",
    "                     validation_data=([data_1_val, data_2_val, f_val], labels_val),\n",
    "                     ## Changed Epoch from 15 --> 25\n",
    "                     epochs=25, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "    model.load_weights(best_model_path)\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "\n",
    "    preds = model.predict([test_data_1, test_data_2, features_test], batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    submission = pd.DataFrame({\"test_id\": test[\"test_id\"], \"is_duplicate\": preds.ravel()})\n",
    "    submission.to_csv(\"../predictions/____preds\" + str(model_count) + \".csv\", index=False)\n",
    "\n",
    "    model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n",
      "Train on 17699 samples, validate on 1105 samples\n",
      "Epoch 1/25\n",
      "17699/17699 [==============================] - 162s 9ms/step - loss: 0.5197 - val_loss: 0.4423\n",
      "Epoch 2/25\n",
      "17699/17699 [==============================] - 150s 8ms/step - loss: 0.4247 - val_loss: 0.3673\n",
      "Epoch 3/25\n",
      "17699/17699 [==============================] - 138s 8ms/step - loss: 0.3450 - val_loss: 0.3202\n",
      "Epoch 4/25\n",
      "17699/17699 [==============================] - 181s 10ms/step - loss: 0.2705 - val_loss: 0.2814\n",
      "Epoch 5/25\n",
      "17699/17699 [==============================] - 163s 9ms/step - loss: 0.2124 - val_loss: 0.2395\n",
      "Epoch 6/25\n",
      "17699/17699 [==============================] - 162s 9ms/step - loss: 0.1606 - val_loss: 0.2035\n",
      "Epoch 7/25\n",
      "17699/17699 [==============================] - 158s 9ms/step - loss: 0.1173 - val_loss: 0.5565\n",
      "Epoch 8/25\n",
      "17699/17699 [==============================] - 150s 8ms/step - loss: 0.1243 - val_loss: 0.4325\n",
      "Epoch 9/25\n",
      "17699/17699 [==============================] - 147s 8ms/step - loss: 0.0890 - val_loss: 0.2839\n",
      "Epoch 10/25\n",
      "17699/17699 [==============================] - 181s 10ms/step - loss: 0.0685 - val_loss: 0.2553\n",
      "Epoch 11/25\n",
      "17699/17699 [==============================] - 156s 9ms/step - loss: 0.0561 - val_loss: 0.2162\n",
      "0 validation loss: 0.2035363215936255\n",
      "MODEL: 1\n",
      "Train on 17699 samples, validate on 1105 samples\n",
      "Epoch 1/25\n",
      "17699/17699 [==============================] - 149s 8ms/step - loss: 0.5289 - val_loss: 0.4578\n",
      "Epoch 2/25\n",
      "17699/17699 [==============================] - 133s 8ms/step - loss: 0.4597 - val_loss: 0.5291\n",
      "Epoch 3/25\n",
      "17699/17699 [==============================] - 133s 8ms/step - loss: 0.4191 - val_loss: 0.4109\n",
      "Epoch 4/25\n",
      "17699/17699 [==============================] - 152s 9ms/step - loss: 0.3933 - val_loss: 0.5242\n",
      "Epoch 5/25\n",
      "17699/17699 [==============================] - 145s 8ms/step - loss: 0.3639 - val_loss: 0.3293\n",
      "Epoch 6/25\n",
      "17699/17699 [==============================] - 144s 8ms/step - loss: 0.3303 - val_loss: 0.3237\n",
      "Epoch 7/25\n",
      "17699/17699 [==============================] - 138s 8ms/step - loss: 0.2945 - val_loss: 0.3089\n",
      "Epoch 8/25\n",
      "17699/17699 [==============================] - 134s 8ms/step - loss: 0.2586 - val_loss: 0.3494\n",
      "Epoch 9/25\n",
      "17699/17699 [==============================] - 136s 8ms/step - loss: 0.2309 - val_loss: 0.2077\n",
      "Epoch 10/25\n",
      "17699/17699 [==============================] - 148s 8ms/step - loss: 0.1982 - val_loss: 0.3753\n",
      "Epoch 11/25\n",
      "17699/17699 [==============================] - 152s 9ms/step - loss: 0.1775 - val_loss: 0.2934\n",
      "Epoch 12/25\n",
      "17699/17699 [==============================] - 142s 8ms/step - loss: 0.1624 - val_loss: 0.2741\n",
      "Epoch 13/25\n",
      "17699/17699 [==============================] - 146s 8ms/step - loss: 0.1313 - val_loss: 0.1515\n",
      "Epoch 14/25\n",
      "17699/17699 [==============================] - 140s 8ms/step - loss: 0.1256 - val_loss: 0.2384\n",
      "Epoch 15/25\n",
      "17699/17699 [==============================] - 137s 8ms/step - loss: 0.0996 - val_loss: 0.1218\n",
      "Epoch 16/25\n",
      "17699/17699 [==============================] - 141s 8ms/step - loss: 0.0882 - val_loss: 0.2727\n",
      "Epoch 17/25\n",
      "17699/17699 [==============================] - 137s 8ms/step - loss: 0.0802 - val_loss: 0.0982\n",
      "Epoch 18/25\n",
      "17699/17699 [==============================] - 144s 8ms/step - loss: 0.0698 - val_loss: 0.1233\n",
      "Epoch 19/25\n",
      "17699/17699 [==============================] - 149s 8ms/step - loss: 0.0580 - val_loss: 0.1122\n",
      "Epoch 20/25\n",
      "17699/17699 [==============================] - 147s 8ms/step - loss: 0.0465 - val_loss: 0.1042\n",
      "Epoch 21/25\n",
      "17699/17699 [==============================] - 141s 8ms/step - loss: 0.0474 - val_loss: 0.1150\n",
      "Epoch 22/25\n",
      "17699/17699 [==============================] - 138s 8ms/step - loss: 0.0486 - val_loss: 0.0963\n",
      "Epoch 23/25\n",
      "17699/17699 [==============================] - 141s 8ms/step - loss: 0.0341 - val_loss: 0.1868\n",
      "Epoch 24/25\n",
      "17699/17699 [==============================] - 142s 8ms/step - loss: 0.0387 - val_loss: 0.1353\n",
      "Epoch 25/25\n",
      "17699/17699 [==============================] - 135s 8ms/step - loss: 0.0298 - val_loss: 0.1431\n",
      "1 validation loss: 0.09632369142986531\n",
      "MODEL: 2\n",
      "Train on 17699 samples, validate on 1105 samples\n",
      "Epoch 1/25\n",
      "17699/17699 [==============================] - 140s 8ms/step - loss: 0.5253 - val_loss: 0.4384\n",
      "Epoch 2/25\n",
      "17699/17699 [==============================] - 141s 8ms/step - loss: 0.4179 - val_loss: 0.4097\n",
      "Epoch 3/25\n",
      "17699/17699 [==============================] - 160s 9ms/step - loss: 0.3454 - val_loss: 0.3339\n",
      "Epoch 4/25\n",
      "17699/17699 [==============================] - 141s 8ms/step - loss: 0.2762 - val_loss: 0.3517\n",
      "Epoch 5/25\n",
      "17699/17699 [==============================] - 142s 8ms/step - loss: 0.2141 - val_loss: 0.2127\n",
      "Epoch 6/25\n",
      "17699/17699 [==============================] - 128s 7ms/step - loss: 0.1646 - val_loss: 0.6140\n",
      "Epoch 7/25\n",
      "17699/17699 [==============================] - 152s 9ms/step - loss: 0.1389 - val_loss: 0.2936\n",
      "Epoch 8/25\n",
      "17699/17699 [==============================] - 147s 8ms/step - loss: 0.1023 - val_loss: 0.5291\n",
      "Epoch 9/25\n",
      "17699/17699 [==============================] - 165s 9ms/step - loss: 0.0977 - val_loss: 0.6584\n",
      "Epoch 10/25\n",
      "17699/17699 [==============================] - 175s 10ms/step - loss: 0.0773 - val_loss: 0.3598\n",
      "2 validation loss: 0.2127425126090848\n",
      "MODEL: 3\n",
      "Train on 17699 samples, validate on 1105 samples\n",
      "Epoch 1/25\n",
      "17699/17699 [==============================] - 168s 10ms/step - loss: 0.5291 - val_loss: 0.4649\n",
      "Epoch 2/25\n",
      "17699/17699 [==============================] - 145s 8ms/step - loss: 0.4532 - val_loss: 0.4493\n",
      "Epoch 3/25\n",
      "17699/17699 [==============================] - 128s 7ms/step - loss: 0.4048 - val_loss: 0.6368\n",
      "Epoch 4/25\n",
      "17699/17699 [==============================] - 130s 7ms/step - loss: 0.3653 - val_loss: 0.4832\n",
      "Epoch 5/25\n",
      "17699/17699 [==============================] - 141s 8ms/step - loss: 0.3131 - val_loss: 0.2753\n",
      "Epoch 6/25\n",
      "17699/17699 [==============================] - 131s 7ms/step - loss: 0.2697 - val_loss: 0.5065\n",
      "Epoch 7/25\n",
      "17699/17699 [==============================] - 129s 7ms/step - loss: 0.2194 - val_loss: 0.5891\n",
      "Epoch 8/25\n",
      "17699/17699 [==============================] - 138s 8ms/step - loss: 0.1814 - val_loss: 0.3999\n",
      "Epoch 9/25\n",
      "17699/17699 [==============================] - 139s 8ms/step - loss: 0.1733 - val_loss: 0.6839\n",
      "Epoch 10/25\n",
      "17699/17699 [==============================] - 135s 8ms/step - loss: 0.1269 - val_loss: 0.2435\n",
      "Epoch 11/25\n",
      "17699/17699 [==============================] - 139s 8ms/step - loss: 0.1136 - val_loss: 0.2621\n",
      "Epoch 12/25\n",
      "17699/17699 [==============================] - 136s 8ms/step - loss: 0.1038 - val_loss: 0.1969\n",
      "Epoch 13/25\n",
      "17699/17699 [==============================] - 150s 8ms/step - loss: 0.0832 - val_loss: 0.2542\n",
      "Epoch 14/25\n",
      "17699/17699 [==============================] - 146s 8ms/step - loss: 0.0662 - val_loss: 0.2410\n",
      "Epoch 15/25\n",
      "17699/17699 [==============================] - 131s 7ms/step - loss: 0.0576 - val_loss: 0.5472\n",
      "Epoch 16/25\n",
      "17699/17699 [==============================] - 146s 8ms/step - loss: 0.0508 - val_loss: 0.1572\n",
      "Epoch 17/25\n",
      "17699/17699 [==============================] - 160s 9ms/step - loss: 0.0353 - val_loss: 0.1573\n",
      "Epoch 18/25\n",
      "17699/17699 [==============================] - 190s 11ms/step - loss: 0.0307 - val_loss: 0.2006\n",
      "Epoch 19/25\n",
      "17699/17699 [==============================] - 189s 11ms/step - loss: 0.0301 - val_loss: 0.1698\n",
      "Epoch 20/25\n",
      "17699/17699 [==============================] - 171s 10ms/step - loss: 0.0272 - val_loss: 0.1659\n",
      "Epoch 21/25\n",
      "17699/17699 [==============================] - 169s 10ms/step - loss: 0.0281 - val_loss: 0.1712\n",
      "3 validation loss: 0.15724559207847216\n",
      "MODEL: 4\n",
      "Train on 17700 samples, validate on 1105 samples\n",
      "Epoch 1/25\n",
      "17700/17700 [==============================] - 197s 11ms/step - loss: 0.5221 - val_loss: 0.4725\n",
      "Epoch 2/25\n",
      "17700/17700 [==============================] - 162s 9ms/step - loss: 0.4373 - val_loss: 0.4994\n",
      "Epoch 3/25\n",
      "17700/17700 [==============================] - 143s 8ms/step - loss: 0.3913 - val_loss: 0.4829\n",
      "Epoch 4/25\n",
      "17700/17700 [==============================] - 139s 8ms/step - loss: 0.3420 - val_loss: 0.3539\n",
      "Epoch 5/25\n",
      "17700/17700 [==============================] - 144s 8ms/step - loss: 0.2904 - val_loss: 0.5691\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17700/17700 [==============================] - 142s 8ms/step - loss: 0.2454 - val_loss: 0.2324\n",
      "Epoch 7/25\n",
      "17700/17700 [==============================] - 147s 8ms/step - loss: 0.1897 - val_loss: 0.3404\n",
      "Epoch 8/25\n",
      "17700/17700 [==============================] - 153s 9ms/step - loss: 0.1560 - val_loss: 0.8078\n",
      "Epoch 9/25\n",
      "17700/17700 [==============================] - 157s 9ms/step - loss: 0.1306 - val_loss: 0.1256\n",
      "Epoch 10/25\n",
      "17700/17700 [==============================] - 155s 9ms/step - loss: 0.0992 - val_loss: 0.1220\n",
      "Epoch 11/25\n",
      "17700/17700 [==============================] - 156s 9ms/step - loss: 0.0861 - val_loss: 0.1615\n",
      "Epoch 12/25\n",
      "17700/17700 [==============================] - 154s 9ms/step - loss: 0.0782 - val_loss: 0.4769\n",
      "Epoch 13/25\n",
      "17700/17700 [==============================] - 141s 8ms/step - loss: 0.0719 - val_loss: 0.0960\n",
      "Epoch 14/25\n",
      "17700/17700 [==============================] - 141s 8ms/step - loss: 0.0493 - val_loss: 0.0936\n",
      "Epoch 15/25\n",
      "17700/17700 [==============================] - 147s 8ms/step - loss: 0.0526 - val_loss: 0.1090\n",
      "Epoch 16/25\n",
      "17700/17700 [==============================] - 131s 7ms/step - loss: 0.0428 - val_loss: 1.2915\n",
      "Epoch 17/25\n",
      "17700/17700 [==============================] - 129s 7ms/step - loss: 0.0543 - val_loss: 0.1460\n",
      "Epoch 18/25\n",
      "17700/17700 [==============================] - 128s 7ms/step - loss: 0.0349 - val_loss: 0.1088\n",
      "Epoch 19/25\n",
      "17700/17700 [==============================] - 127s 7ms/step - loss: 0.0263 - val_loss: 0.0873\n",
      "Epoch 20/25\n",
      "17700/17700 [==============================] - 127s 7ms/step - loss: 0.0263 - val_loss: 0.0871\n",
      "Epoch 21/25\n",
      "17700/17700 [==============================] - 128s 7ms/step - loss: 0.0244 - val_loss: 0.1028\n",
      "Epoch 22/25\n",
      "17700/17700 [==============================] - 129s 7ms/step - loss: 0.0247 - val_loss: 0.1029\n",
      "Epoch 23/25\n",
      "17700/17700 [==============================] - 128s 7ms/step - loss: 0.0217 - val_loss: 0.3088\n",
      "Epoch 24/25\n",
      "17700/17700 [==============================] - 127s 7ms/step - loss: 0.0329 - val_loss: 0.5633\n",
      "Epoch 25/25\n",
      "17700/17700 [==============================] - 128s 7ms/step - loss: 0.0484 - val_loss: 0.1175\n",
      "4 validation loss: 0.0871134019949857\n"
     ]
    }
   ],
   "source": [
    "model_count = 0\n",
    "for idx_train, idx_val in skf.split(train[\"is_duplicate\"], train[\"is_duplicate\"]):\n",
    "    print(\"MODEL:\", model_count)\n",
    "#     data_1_train = data_1[idx_train]\n",
    "#     data_2_train = data_2[idx_train]\n",
    "#     labels_train = labels[idx_train]\n",
    "#     f_train = features_train_1[idx_train]\n",
    "\n",
    "    data_1_train = data_1[idx_train]\n",
    "    data_2_train = data_2[idx_train]\n",
    "    labels_train = labels[idx_train]\n",
    "    f_train = features_train[idx_train]\n",
    "\n",
    "#     data_1_val = data_1[idx_val]\n",
    "#     data_2_val = data_2[idx_val]\n",
    "#     labels_val = labels[idx_val]\n",
    "#     f_val = features_train_1[idx_val]\n",
    "\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    lstm_layer = LSTM(96, recurrent_dropout=0.2)\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    features_input = Input(shape=(f_train.shape[1],), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.25)(features_dense)\n",
    "    \n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(embedded_sequences_1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(embedded_sequences_2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(embedded_sequences_1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(embedded_sequences_2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(embedded_sequences_1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(embedded_sequences_2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(embedded_sequences_1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(embedded_sequences_2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(embedded_sequences_1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(embedded_sequences_2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(embedded_sequences_1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(embedded_sequences_2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "    \n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    \n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    merged_temp = concatenate([diff, mul])\n",
    "    merged_temp = Dense(150, activation=\"relu\")(merged_temp)\n",
    "    merged_temp = Dropout(0.3)(merged_temp)\n",
    "    merged_temp = BatchNormalization()(merged_temp)\n",
    "    \n",
    "    merged = concatenate([merged_temp, merged])\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\")\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    best_model_path = \"best_model\" + str(model_count) + \".h5\"\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    hist = model.fit([data_1_train, data_2_train, f_train], labels_train,\n",
    "                     validation_data=([data_1_val, data_2_val, f_val], labels_val),\n",
    "                     epochs=25, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "    model.load_weights(best_model_path)\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "\n",
    "    #preds = model.predict([test_data_1, test_data_2, features_test_1], batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "#     submission = pd.DataFrame({\"test_id\": test[\"test_id\"], \"is_duplicate\": preds.ravel()})\n",
    "#     submission.to_csv(\"predictions/preds\" + str(model_count) + \".csv\", index=False)\n",
    "\n",
    "    model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'Epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e97479b78e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m acc = pd.DataFrame({'epoch': [ i + 1 for i in hist.history.Epoch],\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     'validation': hist.hist['val_acc']})\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'Epoch'"
     ]
    }
   ],
   "source": [
    "acc = pd.DataFrame({'epoch': [ i + 1 for i in hist.history.Epoch],\n",
    "                    'training': hist.history['acc'],\n",
    "                    'validation': hist.hist['val_acc']})\n",
    "ax = acc.iloc[:,:].plot(x='epoch', figsize={5,8}, grid=True)\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim([0.0,1.0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = pd.DataFrame({'epoch': [ i + 1 for i in hist.epoch]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "for i in hist.epoch:\n",
    "    i += 1 \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4724680963684531, 0.49936987490675566, 0.48286572646231674, 0.35387554934661314, 0.569096114732561, 0.23240264195662277, 0.3403627381065852, 0.8077584999718818, 0.125603928123664, 0.12198892172375415, 0.1615455344521622, 0.4769171592336974, 0.09603751553938938, 0.09355901419037607, 0.10902161216574018, 1.2914902419526113, 0.1459985309866219, 0.10877160160250254, 0.08728121393001996, 0.0871134019949857, 0.10282279472814966, 0.10286479812132288, 0.30883342378279743, 0.5633177786930654, 0.1175417298611203]\n"
     ]
    }
   ],
   "source": [
    "print(hist.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-af790f962a44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m summary_stats = pd.DataFrame({'epoch': [ i + 1 for i in hist.epoch ],\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#                               'train_acc': hist.history['acc'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                               \u001b[0;34m'valid_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                               \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               'valid_loss': hist.history['val_loss']})\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_acc'"
     ]
    }
   ],
   "source": [
    "summary_stats = pd.DataFrame({'epoch': [ i + 1 for i in hist.epoch ],\n",
    "#                               'train_acc': hist.history['acc'],\n",
    "#                               'valid_acc': hist.history['val_acc'],\n",
    "                              'train_loss': hist.history['loss'],\n",
    "                              'valid_loss': hist.history['val_loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = summary_stats.plot(x='epoch', figsize={5,8}, grid=True)\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim([0.0,1.0]);\n",
    "%matplotlib inline\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14fefc048>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14fece748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_stats.plot(x='epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
